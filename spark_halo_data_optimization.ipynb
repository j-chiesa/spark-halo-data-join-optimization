{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a4e418-1924-459a-b93b-7d040260253a",
   "metadata": {},
   "source": [
    "## 1. Set Up Spark Session & Configuration\n",
    "This section initializes the Spark session, sets the configuration to disable automatic broadcast joins (which can be inefficient with large datasets), and creates a new database if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d47fa14-bdc7-4b13-83a4-abf9e333a863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Halo\").getOrCreate()\n",
    "\n",
    "# Disable automatic broadcast joins to optimize performance\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "# Create a database for the Halo dataset\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS halo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981520c-13c5-45f2-a555-c4eb773d5525",
   "metadata": {},
   "source": [
    "## 2. Load and Bucket the Data\n",
    "This section loads the CSV files into DataFrames and buckets them on the `match_id` column with 16 buckets for optimized joins. The data is then saved as Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a4c872c-4e57-491f-8ecd-21c3234d2f90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/25 12:21:41 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load and bucket the 'matches' data\n",
    "df_matches = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/home/iceberg/data/matches.csv\")\n",
    "df_matches.write.mode(\"overwrite\").format(\"iceberg\").bucketBy(16, \"match_id\").saveAsTable(\"halo.matches_bucketed\")\n",
    "\n",
    "# Load and bucket the 'match_details' data\n",
    "df_match_details = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/home/iceberg/data/match_details.csv\")\n",
    "df_match_details.write.mode(\"overwrite\").format(\"iceberg\").bucketBy(16, \"match_id\").saveAsTable(\"halo.match_details_bucketed\")\n",
    "\n",
    "# Load and bucket the 'medals_matches_players' data\n",
    "df_medals_matches_players = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/home/iceberg/data/medals_matches_players.csv\")\n",
    "df_medals_matches_players.write.mode(\"overwrite\").format(\"iceberg\").bucketBy(16, \"match_id\").saveAsTable(\"halo.medals_matches_players_bucketed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ff977-48af-429e-ae0c-c4c3cab6522d",
   "metadata": {},
   "source": [
    "## 3. Bucketed Joins to Combine Data\n",
    "This section joins the bucketed tables (`matches`, `match_details`, and `medals_matches_players`) based on the `match_id` and `player_gamertag`. The join is performed to combine all relevant information from the three tables into a unified dataset. After the join, a temporary view `vw_halo_data` is created, making it available for SQL queries later in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b8705e7-35d3-48cf-95de-deef74e035d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bucketed tables for joining\n",
    "df_matches_bucketed = spark.read.table(\"halo.matches_bucketed\")\n",
    "df_match_details_bucketed = spark.read.table(\"halo.match_details_bucketed\")\n",
    "df_medals_matches_players_bucketed = spark.read.table(\"halo.medals_matches_players_bucketed\")\n",
    "\n",
    "# Create temporary views for SQL queries\n",
    "df_matches_bucketed.createOrReplaceTempView(\"vw_matches_bucketed\")\n",
    "df_match_details_bucketed.createOrReplaceTempView(\"vw_match_details_bucketed\")\n",
    "df_medals_matches_players_bucketed.createOrReplaceTempView(\"vw_medals_matches_players_bucketed\")\n",
    "\n",
    "# Join the bucketed tables to create a unified dataset\n",
    "df_joined = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        m.match_id, \n",
    "        m.mapid, \n",
    "        m.is_team_game, \n",
    "        m.playlist_id, \n",
    "        m.game_variant_id, \n",
    "        m.is_match_over, \n",
    "        m.completion_date, \n",
    "        m.match_duration, \n",
    "        m.game_mode, \n",
    "        m.map_variant_id,\n",
    "        md.player_gamertag,\n",
    "        md.previous_spartan_rank,\n",
    "        md.spartan_rank,\n",
    "        md.previous_total_xp,\n",
    "        md.total_xp,\n",
    "        md.previous_csr_tier,\n",
    "        md.previous_csr_designation,\n",
    "        md.previous_csr,\n",
    "        md.previous_csr_percent_to_next_tier,\n",
    "        md.previous_csr_rank,\n",
    "        md.current_csr_tier,\n",
    "        md.current_csr_designation,\n",
    "        md.current_csr,\n",
    "        md.current_csr_percent_to_next_tier,\n",
    "        md.current_csr_rank,\n",
    "        md.player_rank_on_team,\n",
    "        md.player_finished,\n",
    "        md.player_average_life,\n",
    "        md.player_total_kills,\n",
    "        md.player_total_headshots,\n",
    "        md.player_total_weapon_damage,\n",
    "        md.player_total_shots_landed,\n",
    "        md.player_total_melee_kills,\n",
    "        md.player_total_melee_damage,\n",
    "        md.player_total_assassinations,\n",
    "        md.player_total_ground_pound_kills,\n",
    "        md.player_total_shoulder_bash_kills,\n",
    "        md.player_total_grenade_damage,\n",
    "        md.player_total_power_weapon_damage,\n",
    "        md.player_total_power_weapon_grabs,\n",
    "        md.player_total_deaths,\n",
    "        md.player_total_assists,\n",
    "        md.player_total_grenade_kills,\n",
    "        mp.medal_id,\n",
    "        mp.count AS medal_count\n",
    "    FROM vw_matches_bucketed m\n",
    "    JOIN vw_match_details_bucketed md ON m.match_id = md.match_id\n",
    "    JOIN vw_medals_matches_players_bucketed mp ON md.match_id = mp.match_id AND md.player_gamertag = mp.player_gamertag\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb2193-0ed8-4acb-84e8-255d8d7523ff",
   "metadata": {},
   "source": [
    "## 4. Broadcast Joins for Small Tables\n",
    "This section broadcasts the `maps` and `medals` tables for efficient joining with the large `df_joined` DataFrame. The joined data is stored in a temporary view for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42517db3-4f7c-450f-8ac2-06e47c8de859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 'maps' and 'medals' data and broadcast for efficient joins\n",
    "df_maps = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/home/iceberg/data/maps.csv\")\n",
    "df_maps = df_maps.withColumnRenamed(\"description\", \"map_description\").withColumnRenamed(\"name\", \"map_name\")\n",
    "df_maps_broadcasted = broadcast(df_maps)\n",
    "\n",
    "df_medals = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/home/iceberg/data/medals.csv\")\n",
    "df_medals = df_medals.withColumnRenamed(\"description\", \"medal_description\").withColumnRenamed(\"name\", \"medal_name\")\n",
    "df_medals_broadcasted = broadcast(df_medals)\n",
    "\n",
    "# Join broadcasted tables with the main dataset\n",
    "df_halo_data = df_joined.join(df_medals_broadcasted, on=\"medal_id\", how=\"inner\").join(df_maps_broadcasted, on=\"mapid\", how=\"inner\")\n",
    "df_halo_data.createOrReplaceTempView(\"vw_halo_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c0544-279b-4471-b794-596bd7dfac72",
   "metadata": {},
   "source": [
    "## 5. Aggregated Queries\n",
    "This section runs several aggregate queries to answer the questions regarding the most kills per player, the most played playlist, the most played map, and the map with the most Killing Spree medals. All results are obtained using `spark.sql()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4181416a-45b2-463d-bf1c-f6a04e778795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|player_gamertag|avg_total_kills|\n",
      "+---------------+---------------+\n",
      "|   gimpinator14|          109.0|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|         playlist_id|times_played|\n",
      "+--------------------+------------+\n",
      "|f72e0ef0-7c4a-430...|        7640|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|               mapid|times_played|\n",
      "+--------------------+------------+\n",
      "|c7edbf0f-f206-11e...|        7032|\n",
      "+--------------------+------------+\n",
      "\n",
      "+--------------------+-------------------+\n",
      "|               mapid|killing_spree_total|\n",
      "+--------------------+-------------------+\n",
      "|c7edbf0f-f206-11e...|               6734|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Query 1: Player with the most kills per game\n",
    "most_kills_query = \"\"\"\n",
    "SELECT \n",
    "    player_gamertag,\n",
    "    AVG(player_total_kills) AS avg_total_kills\n",
    "FROM vw_halo_data\n",
    "GROUP BY player_gamertag\n",
    "ORDER BY avg_total_kills DESC\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "most_kills_result = spark.sql(most_kills_query).show()\n",
    "\n",
    "# Query 2: Playlist played the most\n",
    "most_played_playlist_query = \"\"\"\n",
    "WITH matches_playlists AS (\n",
    "    SELECT \n",
    "        match_id,\n",
    "        playlist_id\n",
    "    FROM vw_halo_data\n",
    "    GROUP BY match_id, playlist_id\n",
    ")\n",
    "SELECT \n",
    "    playlist_id,\n",
    "    COUNT(playlist_id) AS times_played\n",
    "FROM matches_playlists\n",
    "GROUP BY playlist_id\n",
    "ORDER BY times_played DESC\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "most_played_playlist_result = spark.sql(most_played_playlist_query).show()\n",
    "\n",
    "# Query 3: Map played the most\n",
    "most_played_map_query = \"\"\"\n",
    "WITH matches_maps AS (\n",
    "    SELECT \n",
    "        match_id,\n",
    "        mapid\n",
    "    FROM vw_halo_data\n",
    "    GROUP BY match_id, mapid\n",
    ")\n",
    "SELECT \n",
    "    mapid,\n",
    "    COUNT(mapid) AS times_played\n",
    "FROM matches_maps\n",
    "GROUP BY mapid\n",
    "ORDER BY times_played DESC\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "most_played_map_result = spark.sql(most_played_map_query).show()\n",
    "\n",
    "# Query 4: Map with the most Killing Spree medals\n",
    "killing_spree_query = \"\"\"\n",
    "SELECT \n",
    "    mapid,\n",
    "    COUNT(classification) AS killing_spree_total\n",
    "FROM vw_halo_data\n",
    "WHERE classification = 'KillingSpree'\n",
    "GROUP BY mapid\n",
    "ORDER BY killing_spree_total DESC\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "killing_spree_result = spark.sql(killing_spree_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b60a0cc-d50c-4926-b565-7fe44295b4fb",
   "metadata": {},
   "source": [
    "## 6. Data Sorting\n",
    "This section sorts the data within partitions based on several columns to reduce data size, particularly for columns with low cardinality like `playlist_id` and `mapid`. The sorted and unsorted versions of the data are saved to separate tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a0a2b15-8c2a-4abd-a8cd-bcb7625fec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sort the data within partitions to optimize size\n",
    "df_sorted = df_halo_data.sortWithinPartitions(\n",
    "    col(\"match_id\"), \n",
    "    col(\"player_gamertag\"), \n",
    "    col(\"medal_id\"),\n",
    "    col(\"game_variant_id\"), \n",
    "    col(\"playlist_id\"), \n",
    "    col(\"mapid\")\n",
    ")\n",
    "\n",
    "# Write both sorted and unsorted datasets to separate tables\n",
    "df_halo_data.write.mode(\"overwrite\").option(\"inferSchema\", \"true\").saveAsTable(\"halo.halo_data_unsorted\")\n",
    "df_sorted.write.mode(\"overwrite\").option(\"inferSchema\", \"true\").saveAsTable(\"halo.halo_data_sorted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b06fe3-f0c1-4313-9048-5769e00ac26d",
   "metadata": {},
   "source": [
    "## 7. Data Size Comparison\n",
    "This section compares the file sizes and number of files between the sorted and unsorted versions of the dataset. This helps identify the impact of sorting on data storage efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25d3ddbf-03d5-414c-b3d9-00a4e48d83bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+\n",
      "|    size|num_files|data_type|\n",
      "+--------+---------+---------+\n",
      "|22187205|       13|   sorted|\n",
      "|22253706|       13| unsorted|\n",
      "+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare the file sizes of sorted and unsorted data\n",
    "file_size_comparison_query = \"\"\"\n",
    "SELECT SUM(file_size_in_bytes) AS size, COUNT(1) AS num_files, 'sorted' AS data_type\n",
    "FROM halo.halo_data_sorted.files\n",
    "UNION ALL\n",
    "SELECT SUM(file_size_in_bytes) AS size, COUNT(1) AS num_files, 'unsorted' AS data_type\n",
    "FROM halo.halo_data_unsorted.files\n",
    "\"\"\"\n",
    "file_size_comparison_result = spark.sql(file_size_comparison_query).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
